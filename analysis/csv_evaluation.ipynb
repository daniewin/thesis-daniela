{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted, original source: ISEAD project (HITeC e.V.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<br>\n",
    "This file is for evaluating the TP, TN, FP, FN, Precision, Recall and F1 Scores of BBox Predictions. Those scores are calculated on an image level and on a BBox level.<br>\n",
    "\n",
    "### Image Level\n",
    "True Positives: Those are images that should have predictions and have some (not specified if bboxes are correct).<br>\n",
    "False Positives: Those are images that should not have predictions but have some.<br>\n",
    "False Negatives: Those are images that should have predictions but have none.<br>\n",
    "True Negatives: Those are images that should not have predictions and have none.<br>\n",
    "<br>\n",
    "\n",
    "### BBox Level:<br>\n",
    "True Positives: Those are bboxes that were correctly predicted.<br>\n",
    "False Positives: Those are bboxes that were unwanted.<br>\n",
    "False Negatives: Those are missing bboxes.<br>\n",
    "True Negatives: This can't appear in our case.<br>\n",
    "<br>\n",
    "This analysis is done via evaluating CSV files containing BBox information from either a BBox or a Segmentation Model. <br>\n",
    "\n",
    "### Steps:<br>\n",
    "At first, we read all necessary <br>\n",
    "    - prediction data from the csv files,<br>\n",
    "    - ground truth data,<br>\n",
    "    - and make sure the prediction data only contains data from the TEST dataset (use Test.txt to filter test images).<br>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# get test data filenames\n",
    "def read_txt(file_path):\n",
    "    test = []\n",
    "    with open(file_path, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            test.append(line.removesuffix('\\n'))\n",
    "    return test\n",
    "\n",
    "# read prediction csv file\n",
    "def read_csv(file_path, test_file, delimiter=';'):\n",
    "    data = []\n",
    "    # test split only relevant for 38k not negative dataset\n",
    "    if test_file:\n",
    "        # get test data filenames\n",
    "        test = read_txt(test_file)\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=delimiter)\n",
    "        headers = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            # filter test data if necessary\n",
    "            bg = \"sea\"\n",
    "            anom = \"bird\"\n",
    "            fs = \".\"\n",
    "            if (test_file and (((row[0].removesuffix('.jpg')).removesuffix('.png')).removesuffix(f'_{anom}{fs}_{bg}_80_0_0.05_0.1_0.1')).removesuffix(\"_majority\") in test) or not test_file:\n",
    "                # confidence threshold\n",
    "                if float(row[7]) > 0.1:\n",
    "                    data.append({headers[i]: (((row[i].removesuffix('.jpg')).removesuffix('.png')).removesuffix(f\"_{anom}{fs}_{bg}_80_0_0.05_0.1_0.1\")).removesuffix(\"_majority\") for i in range(len(headers))})\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_image_dimensions(data, image_id):\n",
    "    for image in data['images']:\n",
    "        if image['id'] == image_id:\n",
    "            return image['width'], image['height']\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# read GT json file (COCO annotations)\n",
    "def read_json(json_file_paths, test_file, neg_test_file):\n",
    "    if test_file:\n",
    "        # get test data filenames\n",
    "        test = read_txt(test_file)\n",
    "        neg_test = read_txt(neg_test_file)\n",
    "    annotations_seg = []\n",
    "    annotations_saa = []\n",
    "    for i in range(len(json_file_paths)):\n",
    "        with open(json_file_paths[i], 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        image_id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n",
    "        print(data['annotations'][0])\n",
    "        for annotation in data['annotations']:\n",
    "            \n",
    "            image_id = annotation['image_id']\n",
    "            image_width, image_height = get_image_dimensions(data, image_id)\n",
    "\n",
    "            bbox_dict = {}\n",
    "            bbox_dict['image'] = (image_id_to_filename.get(annotation['image_id'], \"Unknown\")).removesuffix(\".jpg\")\n",
    "            \n",
    "            bbox_dict['xmin'] = annotation['bbox'][0]/image_width\n",
    "            bbox_dict['ymin'] = annotation['bbox'][1]/image_height\n",
    "            bbox_dict['xmax'] = (annotation['bbox'][0] + annotation['bbox'][2])/image_width\n",
    "            bbox_dict['ymax'] = (annotation['bbox'][1] + annotation['bbox'][3])/image_height\n",
    "            if i == 0: \n",
    "                if (test_file and bbox_dict['image'] in test and bbox_dict['image'] not in neg_test) or not test_file:\n",
    "                    annotations_seg.append(bbox_dict)\n",
    "\n",
    "            if annotation['category_id'] == 3:\n",
    "                annotations_saa.append(bbox_dict)\n",
    "            \n",
    "    return annotations_seg, annotations_saa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(bbox1, bbox2):\n",
    "    x1 = max(float(bbox1['xmin']), float(bbox2['xmin']))\n",
    "    y1 = max(float(bbox1['ymin']), float(bbox2['ymin']))\n",
    "    x2 = min(float(bbox1['xmax']), float(bbox2['xmax']))\n",
    "    y2 = min(float(bbox1['ymax']), float(bbox2['ymax']))\n",
    "\n",
    "    intersection_area = max(0, x2 - x1 + 0.0001) * max(0, y2 - y1 + 0.0001)\n",
    "\n",
    "    bbox1_area = (float(bbox1['xmax']) - float(bbox1['xmin']) + 0.0001) * (float(bbox1['ymax']) - float(bbox1['ymin']) + 0.0001)\n",
    "    bbox2_area = (float(bbox2['xmax']) - float(bbox2['xmin']) + 0.0001) * (float(bbox2['ymax']) - float(bbox2['ymin']) + 0.0001)\n",
    "\n",
    "    union_area = bbox1_area + bbox2_area - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBox Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_metrics_bbox_level(ground_truth, predictions, iou_threshold):\n",
    "    true_positive_predictions = []\n",
    "    false_positive_predictions = []\n",
    "    for pred in predictions:\n",
    "        false_positive_predictions.append(pred)\n",
    "    false_negative_predictions = []\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    confidence_tp = []\n",
    "    confidence_fp = []\n",
    "\n",
    "    for gt_bbox in ground_truth:\n",
    "        found_matching_prediction = False\n",
    "        for pred_bbox in predictions:\n",
    "            # only if image is also the same\n",
    "            if pred_bbox['image'] == gt_bbox['image']:\n",
    "                iou = calculate_iou(gt_bbox, pred_bbox)\n",
    "                if iou >= iou_threshold:\n",
    "                    true_positives += 1\n",
    "                    true_positive_predictions.append(gt_bbox)\n",
    "                    if pred_bbox in false_positive_predictions:\n",
    "                        false_positive_predictions.remove(pred_bbox)\n",
    "                    confidence_tp.append(float(pred_bbox['confidence']))\n",
    "                    found_matching_prediction = True\n",
    "                    break\n",
    "\n",
    "        if not found_matching_prediction:\n",
    "            false_negative_predictions.append(gt_bbox)\n",
    "            false_negatives += 1\n",
    "\n",
    "    for pred_bbox in false_positive_predictions:\n",
    "        confidence_fp.append(float(pred_bbox['confidence']))\n",
    "\n",
    "    \n",
    "    false_positives = len(false_positive_predictions)\n",
    "    \n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\n",
    "    return ({\n",
    "        'tp': true_positives,\n",
    "        'fp': false_positives,\n",
    "        'fn': false_negatives,\n",
    "        'tn': 0,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }, [confidence_tp, confidence_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_image_level(ground_truth, predictions, num_total_images):\n",
    "\n",
    "    # set of unique image filenames mentioned in files (images without detections are not listed)\n",
    "    gt_images = set([bbox['image'] for bbox in ground_truth])\n",
    "    gt = len(gt_images)\n",
    "    pred_images = set([bbox['image'] for bbox in predictions])\n",
    "    pred = len(pred_images)\n",
    "\n",
    "    # get overlap/intersection with AND operator\n",
    "    true_positive_images = gt_images & pred_images\n",
    "    tp = len(true_positive_images)\n",
    "    # get non overlaps \n",
    "    # files that only have detections in prediction\n",
    "    false_positive_images = pred_images - true_positive_images\n",
    "    fp = len(false_positive_images)\n",
    "    # files that only have detections in gt\n",
    "    false_negative_images = gt_images - true_positive_images\n",
    "    fn = len(false_negative_images)\n",
    "\n",
    "    # get true negatives via union with | Operator\n",
    "    tn = num_total_images - len(false_negative_images | true_positive_images | false_positive_images)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for 38k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniela/Documents/Thesis/Data/ISEAD/38k/birds/SegmentationMask/GT_birds/csv_GT/prediction_no_spread.csv\n",
      "/home/daniela/Documents/Thesis/Data/ISEAD/38k/birds/SegmentationMask/student_predictions/GT_birds/GT_birds_student_bird_sea_20_0.05_GT/test_prediction_2.csv\n",
      "GROUND TRUTH {'image': '10k_102869', 'xmin': '0.2', 'ymin': '0.004', 'xmax': '0.22', 'ymax': '0.024', 'obj_id': '0', 'obj_class': 'ObjectClass.UNKNOWN', 'confidence': '1.0'}\n",
      "10365\n",
      "{'image': '10k_102869', 'xmin': '0.18359375', 'ymin': '0.0', 'xmax': '0.234375', 'ymax': '0.03515625', 'obj_id': '0', 'obj_class': 'ObjectClass.UNKNOWN', 'confidence': '1.0'}\n",
      "19585\n"
     ]
    }
   ],
   "source": [
    "# check that prediction csv only contains test dataset\n",
    "\n",
    "\n",
    "# paths to csv \n",
    "predictions_seg_file = '/home/daniela/Documents/Thesis/Data/ISEAD/38k/birds/SegmentationMask/student_predictions/GT_birds/GT_birds_student_bird_sea_20_0.05_GT/test_prediction_2.csv'\n",
    "ground_truth_test_file_csv = '/home/daniela/Documents/Thesis/Data/ISEAD/38k/birds/SegmentationMask/GT_birds/csv_GT/prediction_no_spread.csv'\n",
    "test_file = \"/home/daniela/Documents/Thesis/Data/ISEAD/38k/birds/SegmentationMask/GT/ImageSets/Segmentation/Test.txt\"\n",
    "\n",
    "\n",
    "num_total_images = 3006 # test split\n",
    "iou_threshold = 0.005\n",
    "\n",
    "# read data from csvs\n",
    "ground_truth_seg = read_csv(ground_truth_test_file_csv, test_file)\n",
    "predictions_seg = read_csv(predictions_seg_file, test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(metrics_image_level, metrics_bbox_level):\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"Image Level Evaluation:\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"True Positives:\", metrics_image_level['tp'])\n",
    "    print(\"Those are images that should have predictions and have some (not specified if bboxes are correct).\")\n",
    "    print(\"False Positives:\", metrics_image_level['fp'])\n",
    "    print(\"Those are images that should not have predictions but have some.\")\n",
    "    print(\"False Negatives:\", metrics_image_level['fn'])\n",
    "    print(\"Those are images that should have predictions but have none.\")\n",
    "    print(\"True Negatives:\", metrics_image_level['tn'])\n",
    "    print(\"Those are images that should not have predictions and have none.\")\n",
    "    print(\"Total Images:\", num_total_images)\n",
    "    print()\n",
    "    print(\"Precision:\", metrics_image_level['precision'])\n",
    "    print(\"Recall:\", metrics_image_level['recall'])\n",
    "    print(\"F1 Score:\", metrics_image_level['f1_score'])\n",
    "    print()\n",
    "\n",
    "    print(\"BBox Level Evaluation:\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    print(\"True Positives:\", metrics_bbox_level['tp'])\n",
    "    print(\"Those are bboxes that were correctly predicted.\")\n",
    "    print(\"False Positives:\", metrics_bbox_level['fp'])\n",
    "    print(\"Those are bboxes that were unwanted.\")\n",
    "    print(\"False Negatives:\", metrics_bbox_level['fn'])\n",
    "    print(\"Those are missing bboxes.\")\n",
    "    print(\"True Negatives:\", metrics_bbox_level['tn'])\n",
    "    print(\"This can't appear in our case.\")\n",
    "    print()\n",
    "    \"\"\"\n",
    "    print(\"Precision:\", round(metrics_bbox_level['precision'], ndigits=3))\n",
    "    print(\"Recall:\", round(metrics_bbox_level['recall'], ndigits=3))\n",
    "    print(\"F1 Score:\", round(metrics_bbox_level['f1_score'], ndigits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Images:\n",
      "{'10k_Lachmöwe_SWIM_BIRD_3805208_SK2C107769_t10_cam1', 'task_438_Fluss--Küstenseeschwalbe_FLY_BIRD_660586_HD722173_t11_cam2', 'task_445_Heringsmöwe_FLY_BIRD_653385_HD737376_t10_cam2', 'task_439_Fluss--Küstenseeschwalbe_FLY_BIRD_675887_HD628219_t13_cam1', 'task_445_Heringsmöwe_FLY_BIRD_682922_HD648544_t8_cam1', '10k_Lachmöwe_SWIM_BIRD_3805223_SK2C107769_t10_cam1', 'task_400_Dreizehenmöwe_FLY_BIRD_3620821_HD1700_t5_cam2'}\n",
      "confidence tp: 1.0\n",
      "confidence fp: 1.0\n",
      "\n",
      "----------------\n",
      "| SEGMENTATION |\n",
      "----------------\n",
      "\n",
      "Precision: 0.457\n",
      "Recall: 0.934\n",
      "F1 Score: 0.613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate metrics for BBox\n",
    "#metrics_image_level_saa = calculate_metrics_image_level(ground_truth_saa, predictions_saa, num_total_images)\n",
    "#metrics_bbox_level_saa, confidence_saa = calculate_metrics_bbox_level(ground_truth_saa, predictions_saa, iou_threshold)\n",
    "# calculate metrics for Segmentation\n",
    "metrics_image_level_seg = calculate_metrics_image_level(ground_truth_seg, predictions_seg, num_total_images)\n",
    "metrics_bbox_level_seg, confidence_seg = calculate_metrics_bbox_level(ground_truth_seg, predictions_seg, iou_threshold)\n",
    "\n",
    "# print output Segmentation\n",
    "print()\n",
    "print(\"----------------\")\n",
    "print(\"| SEGMENTATION |\")\n",
    "print(\"----------------\")\n",
    "print()\n",
    "print_output(metrics_image_level_seg, metrics_bbox_level_seg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confidence distribution\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(np.max(confidence_seg[0]))\n",
    "print(np.min(confidence_seg[0]))\n",
    "\n",
    "# plot of what confidence threshold to choose\n",
    "\n",
    "def get_amount(threshold, list):\n",
    "    return np.sum([c < threshold for c in  list])/len(list)\n",
    "\n",
    "confidence_threshold = np.arange(0.0, 1.1, 0.01)\n",
    "amount_tp = [1-get_amount(t, confidence_seg[0]) for t in confidence_threshold]\n",
    "amount_fp = [get_amount(t, confidence_seg[1]) for t in confidence_threshold]\n",
    "plt.plot(confidence_threshold, amount_tp)\n",
    "plt.plot(confidence_threshold, amount_fp)\n",
    "\n",
    "# histograms of confidence\n",
    "\n",
    "#plt.hist(confidence_saa[0], density=True, bins=30)  # density=False would make counts\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.xlabel('Confidence SAA TP')\n",
    "#plt.plot()\n",
    "\n",
    "#plt.hist(confidence_saa[1], density=True, bins=30)  # density=False would make counts\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.xlabel('Confidence SAA FP')\n",
    "#plt.plot()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isead-docker-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
